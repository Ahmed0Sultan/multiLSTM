{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py-CRFsuite NER model\n",
    "\n",
    "this version does not consider capitalization (all words are lowered with `.lower()`), in hopes of creating a case-independent model, for use in situations where case information is not available, such as when using the output of an automatic speech recognition (speech-to-text) system.\n",
    "\n",
    "code modified from:\n",
    "https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pycrfsuite\n",
    "from nltk import pos_tag\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read travel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokens = list(np.load('../00_data/encoded/add_tokens.npy'))\n",
    "X_postags = list(np.load('../00_data/encoded/add_postags.npy'))\n",
    "X_zips = [list(zip(X_tokens[i], X_postags[i])) for i in range(len(X_tokens))]\n",
    "y_nertags = list(np.load('../00_data/encoded/add_nertags.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gazetteers\n",
    "\n",
    "precompiled lists for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lists\n",
    "timewords = ['midnight', 'noon', 'am', 'pm', 'morning', 'afternoon', 'evening', 'night', \"o'clock\"]\n",
    "timewords = [s.lower() for s in timewords]\n",
    "\n",
    "datewords = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'January', 'February',\n",
    "             'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December',\n",
    "             'tomorrow']\n",
    "datewords = [s.lower() for s in datewords]\n",
    "\n",
    "# from transdict finds\n",
    "placewords = ['Euston', 'London', 'Birmingham', 'Street', 'Chicago', 'Wilmslow', 'Macclesfield', 'Stockport',\n",
    "              'Piedmont', 'Dallas', 'Newark', 'Wigan', 'Preston', 'Denver', 'Liverpool', 'Seattle', 'Tokyo',\n",
    "              'Wrexham', 'Richmond' , 'Manchester', 'Crewe', 'Baltimore', 'Ottawa', 'Toronto', 'Vancouver', \n",
    "              'Moscow', 'Boston', 'Edinburgh', 'Oakland', 'Newcastle', 'Durham', 'Lime', 'Taunton', \n",
    "              'Copenhagen', 'Heathrow', 'Helsinki', 'Pittsburg', 'Raleigh', 'Picadilly', 'Watford', 'Hertford',\n",
    "              'Leicester', 'Newton', 'Abbot', 'Greenbay', 'Miami', 'Orlando', 'Washington', 'Dulles', 'Charlotte',\n",
    "              'Tahoe', 'Southbend', 'Springfield', 'York', 'Burbank', 'Syracuse', 'Cleveland', 'Fairbanks',\n",
    "              'Carolina', 'Montreal', 'Wolverhampton', 'Leeds', 'Derby', 'Blackpool', 'Oxenholme', 'Ontario',\n",
    "              'Riyadh', 'Portland', 'Barclay', 'Calgary', 'Bangkok', 'Burigan', 'Nantucket', 'Menlo', 'Cottage',\n",
    "              'Wisconsin', 'Gatwick', 'Singapore', 'Irvine', 'Frankfurt', 'Jersey', 'Columbus', 'Merseyside', \n",
    "              'Fanshawe', 'Essex', 'Stafford', 'Philadelphia', 'Switzerland', 'Denmark', 'Sandestrom',\n",
    "              'Braniff', 'Stockholm', 'Sweden', 'Germany', 'Belgium', 'Brussels', 'Rochester', 'Anchorage', \n",
    "              'California', 'Arkansas', 'England', 'Michigan', 'Detroit', 'Indiana', 'Louisville', 'Ohio',\n",
    "              'Tulsa', 'Indianapolis', 'Milwaukee', 'Oklahoma', 'Colorado', 'Virginia', 'Coxhoe', 'Redwich',\n",
    "              'Camden', 'Leicestershire', 'Cumbria', 'Heswall', 'Wirral', 'Liver', 'Cheshire', 'Goldhampton',\n",
    "              'International', 'Airport', 'Central', 'General', 'St', 'Atlanta', 'Nuneaton', 'Beijing', 'Europe',\n",
    "              'LA', 'ORD', 'SFO', 'JFK', 'LAX', 'SAS', 'SF', 'Pancreas', 'Rugby',\n",
    "              'Palo', 'Green', 'New', 'Lime', 'Orange', 'San', 'Los', 'Hong', 'John', 'Las', 'Saudi', 'Saint',\n",
    "              'Santa', 'Soviet', 'Little', 'Alto', 'Bay', 'Lime' 'Street', 'County', 'Francisco', 'Angeles', \n",
    "              'Diego', 'Jose', 'Bernadino', 'Kong', 'Wayne', 'Vegas', 'Arabia', 'Louis', 'Petersburg', 'Ana', \n",
    "              'Union', 'Rock']\n",
    "placewords = [s.lower() for s in placewords]\n",
    "\n",
    "# from transdict finds\n",
    "companywords = ['United', 'American', 'Virgin', 'Express', 'Delta', 'Trainlines', 'Visa', 'Airlines', 'Travel',\n",
    "                'Hertz', 'Lufthansa', 'Northwest', 'Canadian', 'British', 'Trainline', 'Northwestern',\n",
    "                'Mastercard', 'Airline', 'Southwestern', 'Telesales', 'Merchandising', 'Aeroflight',\n",
    "                'Airways', 'Korean', 'Hotel', 'Alaskan', 'Alaska', 'TWA', 'Eagle', 'Saudia', 'Arabian', \n",
    "                'PanAm', 'CP', 'Air', 'trainlines', 'Continental', 'Cathay', 'Pan', 'Pacific', 'Am']\n",
    "companywords = [s.lower() for s in companywords]\n",
    "\n",
    "# seat-class words\n",
    "ticketwords = ['Coach', 'Business', 'non-stop', 'Round-trip', 'Advance',\n",
    "               'Super', 'Value', 'Return', 'Saver', 'First', 'Class', 'Single']\n",
    "ticketwords = [s.lower() for s in ticketwords]\n",
    "\n",
    "currencies = ['pounds', 'dollars', 'cents', 'dollar']\n",
    "\n",
    "numbers = ['hundred', 'hundreds', 'thousand', 'thousands']\n",
    "\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "# timewords, datewords, placewords, companywords, ticketwords, currencies, numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'wordlength='+str(len(word)),\n",
    "        'wordending[-3:]=' + word[-3:],\n",
    "        'wordending[-2:]=' + word[-2:],\n",
    "        'wordending[-1:]=' + word[-1:],\n",
    "        'postag=' + postag,\n",
    "        'posclass=' + postag[:2],\n",
    "        'word.hasdigit=%s' % (word.find('#')>-1),\n",
    "        'word.istime=%s' % (word in timewords),\n",
    "        'word.isdate=%s' % (word in datewords),\n",
    "        'word.isplace=%s' % (word in placewords),\n",
    "        'word.iscompany=%s' % (word in companywords),\n",
    "        'word.istixtype=%s' % (word in ticketwords),\n",
    "        'word.iscurrency=%s' % (word in currencies),\n",
    "        'word.isnumber=%s' % (word in numbers),\n",
    "        'word.startsvowel=%s' % (word[0] in vowels),\n",
    "        'word.endsvowel=%s' % (word[-1] in vowels),\n",
    "    ]\n",
    "    \n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:wordending[-3:]=' + word1[-3:],\n",
    "            '-1:wordending[-2:]=' + word1[-2:],\n",
    "            '-1:wordending[-1:]=' + word1[-1:],\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:posclass=' + postag1[:2],\n",
    "            '-1:word.hasdigit=%s' % (word1.find('#')>-1),\n",
    "            '-1:word.istime=%s' % (word1 in timewords),\n",
    "            '-1:word.isdate=%s' % (word1 in datewords),\n",
    "            '-1:word.isplace=%s' % (word1 in placewords),\n",
    "            '-1:word.iscompany=%s' % (word1 in companywords),\n",
    "            '-1:word.istixtype=%s' % (word1 in ticketwords),\n",
    "            '-1:word.iscurrency=%s' % (word1 in currencies),\n",
    "            '-1:word.isnumber=%s' % (word1 in numbers),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:wordending[-3:]=' + word1[-3:],\n",
    "            '+1:wordending[-2:]=' + word1[-2:],\n",
    "            '+1:wordending[-1:]=' + word1[-1:],\n",
    "            '+1:postag=' + postag1,\n",
    "            '+1:posclass=' + postag1[:2],\n",
    "            '+1:word.hasdigit=%s' % (word1.find('#')>-1),\n",
    "            '+1:word.istime=%s' % (word1 in timewords),\n",
    "            '+1:word.isdate=%s' % (word1 in datewords),\n",
    "            '+1:word.isplace=%s' % (word1 in placewords),\n",
    "            '+1:word.iscompany=%s' % (word1 in companywords),\n",
    "            '+1:word.istixtype=%s' % (word1 in ticketwords),\n",
    "            '+1:word.iscurrency=%s' % (word1 in currencies),\n",
    "            '+1:word.isnumber=%s' % (word1 in numbers),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(len(X_tokens)*0.9)\n",
    "train_sents = X_zips[:split_idx]\n",
    "test_sents = X_zips[split_idx:]\n",
    "y_train = y_nertags[:split_idx]\n",
    "y_test = y_nertags[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.22 s, sys: 52 ms, total: 2.28 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "X_test = [sent2features(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias',\n",
       " 'wordlength=4',\n",
       " 'wordending[-3:]=hat',\n",
       " 'wordending[-2:]=at',\n",
       " 'wordending[-1:]=t',\n",
       " 'postag=WP',\n",
       " 'posclass=WP',\n",
       " 'word.hasdigit=False',\n",
       " 'word.istime=False',\n",
       " 'word.isdate=False',\n",
       " 'word.isplace=False',\n",
       " 'word.iscompany=False',\n",
       " 'word.istixtype=False',\n",
       " 'word.iscurrency=False',\n",
       " 'word.isnumber=False',\n",
       " 'word.startsvowel=False',\n",
       " 'word.endsvowel=False',\n",
       " 'BOS',\n",
       " '+1:wordending[-3:]=ime',\n",
       " '+1:wordending[-2:]=me',\n",
       " '+1:wordending[-1:]=e',\n",
       " '+1:postag=NN',\n",
       " '+1:posclass=NN',\n",
       " '+1:word.hasdigit=False',\n",
       " '+1:word.istime=False',\n",
       " '+1:word.isdate=False',\n",
       " '+1:word.isplace=False',\n",
       " '+1:word.iscompany=False',\n",
       " '+1:word.istixtype=False',\n",
       " '+1:word.iscurrency=False',\n",
       " '+1:word.isnumber=False']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_sents[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 s, sys: 16 ms, total: 1.67 s\n",
      "Wall time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "    \n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 200,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.1 s, sys: 4 ms, total: 30.1 s\n",
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('../00_data/model/baseline.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create tagger, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f71d4544c88>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('../00_data/model/baseline.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that 's on the ##th of october\n",
      "\n",
      "Predicted: O O O O DAT O DAT\n",
      "Correct:   O O O O DAT O DAT\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "example_sent = test_sents[idx]\n",
    "print(' '.join(sent2tokens(example_sent)), end='\\n\\n')\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(y_test[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.998266078184111\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for idx, sent in enumerate(test_sents):\n",
    "    \n",
    "    preds = tagger.tag(sent2features(sent))\n",
    "    trues = y_test[idx]\n",
    "\n",
    "    for jdx, pred in enumerate(preds):\n",
    "        \n",
    "        if pred == trues[jdx]:\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "            \n",
    "print('acc:', sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerasCRF",
   "language": "python",
   "name": "kerascrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
