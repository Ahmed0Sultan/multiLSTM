{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decode some test sentences\n",
    "\n",
    "save as a csv file for reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/derek/anaconda3/envs/kerasCRF/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset import index_sents\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import Activation, concatenate, Dense, Input, LSTM, Dropout, Embedding\n",
    "from attention import Attention\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.utils import save_load_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_dir' : '../00_data/encoded/',\n",
    "    'model_dir' : '../00_data/model/',\n",
    "    'model_name' : 'alt_combo_model.h5',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main (picklable) class\n",
    "class NameEntityRecognizer():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self._load_data(config['data_dir'])\n",
    "        self.model = self._load_model(config['model_dir']+config['model_name'])\n",
    "        self.nertagset = list(self.ner2idx.keys())\n",
    "        \n",
    "    # load data files\n",
    "    def _load_data(self, datadir):\n",
    "        # load data conversion dictionaries\n",
    "        self.word2idx = np.load(datadir+'word2idx.npy').item()\n",
    "        self.idx2word = np.load(datadir+'idx2word.npy').item()\n",
    "        self.pos2idx = np.load(datadir+'pos2idx.npy').item()\n",
    "        self.idx2pos = np.load(datadir+'idx2pos.npy').item()\n",
    "        self.ner2idx = np.load(datadir+'ner2idx.npy').item()\n",
    "        self.idx2ner = np.load(datadir+'idx2ner.npy').item()\n",
    "        self.sa2idx = np.load(datadir+'sa2idx.npy').item()\n",
    "        self.idx2sa = np.load(datadir+'idx2sa.npy').item()\n",
    "        self.top2idx = np.load(datadir+'top2idx.npy').item()\n",
    "        self.idx2top = np.load(datadir+'idx2top.npy').item()\n",
    "    \n",
    "    # load Keras NER-CRF model\n",
    "    def _load_model(self, modelpath):\n",
    "        # keras model loading\n",
    "        # network hyperparameters\n",
    "        MAX_LENGTH = 20\n",
    "        EMBEDDING_SIZE = 160\n",
    "        POSBEDDING_SIZE = 32\n",
    "        HIDDEN_SIZE = 192\n",
    "        DROPOUTRATE = 0.33\n",
    "\n",
    "        # dict-dependent hyperparameters\n",
    "        MAX_VOCAB = len(self.word2idx.keys())\n",
    "        TAG_VOCAB = len(list(self.idx2pos.keys()))\n",
    "        NER_VOCAB = len(list(self.idx2ner.keys()))\n",
    "        INT_VOCAB = len(list(self.idx2sa.keys()))\n",
    "        TOP_VOCAB = len(list(self.idx2top.keys()))\n",
    "    \n",
    "        # text layers : dense embedding > dropout > bi-LSTM\n",
    "        txt_input = Input(shape=(MAX_LENGTH,), name='txt_input')\n",
    "        txt_embed = Embedding(MAX_VOCAB, EMBEDDING_SIZE, input_length=MAX_LENGTH,\n",
    "                              name='txt_embedding', trainable=False, mask_zero=True)(txt_input)\n",
    "        txt_drpot = Dropout(DROPOUTRATE, name='txt_dropout')(txt_embed)\n",
    "        txt_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                                  name='txt_bidirectional')(txt_drpot)\n",
    "\n",
    "        # pos layers : dense embedding > dropout > bi-LSTM\n",
    "        pos_input = Input(shape=(MAX_LENGTH,), name='pos_input')\n",
    "        pos_embed = Embedding(TAG_VOCAB, POSBEDDING_SIZE, input_length=MAX_LENGTH,\n",
    "                              name='pos_embedding', trainable=True, mask_zero=True)(pos_input)\n",
    "        pos_drpot = Dropout(DROPOUTRATE, name='pos_dropout')(pos_embed)\n",
    "\n",
    "        # merged layers : merge (concat, average...) word and pos > bi-LSTM > bi-LSTM\n",
    "        mrg_cncat = concatenate([txt_lstml, pos_drpot], axis=2)\n",
    "        mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                                  name='mrg_bidirectional_1')(mrg_cncat)\n",
    "\n",
    "        # final NER linear chain CRF layer\n",
    "        crf = CRF(NER_VOCAB, sparse_target=True)\n",
    "        out_ner = crf(mrg_lstml)\n",
    "\n",
    "        # intent network\n",
    "        rnn_intent = Attention(name='int_attention')(mrg_lstml)\n",
    "\n",
    "        # intent\n",
    "        dns_intent = Dense(INT_VOCAB, activation='relu', name='int_dense_1')(rnn_intent)\n",
    "        dns_intent = Dense(INT_VOCAB, name='int_dense_2')(dns_intent)\n",
    "        out_intent = Activation('softmax', name='int_output')(dns_intent)\n",
    "\n",
    "        # topic\n",
    "        dns_top = Dense(TOP_VOCAB, activation='relu', name='top_dense_1')(rnn_intent)\n",
    "        dns_intent = Dense(INT_VOCAB, name='int_dense_2')(dns_top)\n",
    "        out_top = Activation('softmax', name='top_output')(dns_top)\n",
    "\n",
    "        model = Model(inputs=[txt_input, pos_input], outputs=[out_ner, out_intent, out_top])\n",
    "\n",
    "        # save for later (duhhh...)\n",
    "        self.MAX_LENGTH = MAX_LENGTH\n",
    "\n",
    "        # load model weigghts\n",
    "        save_load_utils.load_all_weights(model, modelpath)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    # tokenize and pos-tag with NLTK\n",
    "    # preprocess: lowercase, strip punct\n",
    "    def _tokenize(self, s):\n",
    "        \n",
    "        for punc in ['.', ',', '!', '?', '-', '\"', \"'\"]:\n",
    "            s = s.replace(punc, '')\n",
    "        \n",
    "        tok_tags = pos_tag(word_tokenize(s.lower()))\n",
    "        s_toks = [t[0] for t in tok_tags]\n",
    "        s_tags = [t[1] for t in tok_tags]\n",
    "        \n",
    "        return s_toks, s_tags\n",
    "    \n",
    "    \n",
    "    # integer-index  and pad sent and tag sequences\n",
    "    def _index_sents(self, s_toks, s_tags):\n",
    "        \n",
    "        X_toks = index_sents([s_toks], self.word2idx)\n",
    "        X_tags = index_sents([s_tags], self.pos2idx)\n",
    "        X_toks = sequence.pad_sequences(X_toks, maxlen=self.MAX_LENGTH, truncating='post', padding='post')\n",
    "        X_tags = sequence.pad_sequences(X_tags, maxlen=self.MAX_LENGTH, truncating='post', padding='post')\n",
    "        \n",
    "        return X_toks, X_tags\n",
    "    \n",
    "    \n",
    "    # convert string to a map using decode()\n",
    "    def _ner_dict(self, toks, ners):\n",
    "        dct = {}\n",
    "        for idx, word in enumerate(toks):\n",
    "            if ners[idx] != 'O':\n",
    "                if ners[idx] in dct.keys():\n",
    "                    dct[ners[idx]] += ' '\n",
    "                    dct[ners[idx]] += word\n",
    "                else:\n",
    "                    dct[ners[idx]] = word\n",
    "        return dct\n",
    "    \n",
    "    \n",
    "    # predict on sentences\n",
    "    def predict(self, s, debug=False):\n",
    "\n",
    "        s_toks, s_tags = self._tokenize(s)\n",
    "        f_toks = s_toks[:]\n",
    "        \n",
    "        for i, w in enumerate(s_toks):\n",
    "            for number in ['1','2','3','4','5','6','7','8','9','0']:\n",
    "                w = w.replace(number, '#')\n",
    "            s_toks[i] = w\n",
    "        \n",
    "        X_toks, X_tags = self._index_sents(s_toks, s_tags)\n",
    "\n",
    "        this_pred = self.model.predict([X_toks, X_tags])\n",
    "\n",
    "        this_nerpred = list(this_pred[0])\n",
    "        this_intpred = this_pred[1]\n",
    "        this_toppred = this_pred[2]\n",
    "\n",
    "        this_nerpred = [np.argmax(p) for p in this_nerpred[0]]\n",
    "        this_intpred = np.argmax(this_intpred[0])\n",
    "        this_toppred = np.argmax(this_toppred[0])\n",
    "\n",
    "        word, prd = [], []\n",
    "\n",
    "        # decode ner-sequence, intents\n",
    "        for idx, wordid in enumerate(X_toks[0][:len(s_toks)]):\n",
    "\n",
    "            if self.idx2word[wordid] != 'PAD' and self.idx2pos[X_tags[0][idx]] != 'PAD':\n",
    "\n",
    "                # decode word (from TRUE sequence)\n",
    "                word.append(f_toks[idx])\n",
    "                # decode prediction\n",
    "                prd.append(self.idx2ner[this_nerpred[idx]])\n",
    "\n",
    "        intent = self.idx2sa[this_intpred]\n",
    "        topic = self.idx2top[this_toppred]\n",
    "        \n",
    "        if debug:\n",
    "            print(word)\n",
    "            print(prd)\n",
    "\n",
    "        return self._ner_dict(word, prd), intent, topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = NameEntityRecognizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do', 'you', 'have', 'any', 'flights', 'from', 'seoul', 'leaving', 'tomorrow']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'GEO', 'O', 'DAT']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'DAT': 'tomorrow', 'GEO': 'seoul'}, 'reqInfo', 'day')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict(\"do you have any flights from Seoul leaving tomorrow?\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'there', 'any', 'flights', 'to', 'london', 'at', '5:30']\n",
      "['O', 'O', 'O', 'O', 'O', 'GEO', 'O', 'TIM']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'GEO': 'london', 'TIM': '5:30'}, 'reqInfo', 'time')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict(\"are there any flights to London at 5:30?\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lets', 'do', 'the', '11:20', 'am', 'flight', 'to', 'tokyo']\n",
      "['O', 'O', 'O', 'TIM', 'TIM', 'O', 'O', 'GEO']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'GEO': 'tokyo', 'TIM': '11:20 am'}, 'state', 'enum')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict(\"let's do the 11:20 am flight to Tokyo\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerasCRF",
   "language": "python",
   "name": "kerascrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
