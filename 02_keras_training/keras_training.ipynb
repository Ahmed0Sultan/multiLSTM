{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset import index_sents\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import Activation, concatenate, Dense, Input, LSTM, Dropout, Embedding\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from gensim.models import Word2Vec\n",
    "from mlxtend.preprocessing import one_hot\n",
    "from embedding import load_vocab\n",
    "from attention.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict GPU usage here\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from npys (see preprocessing.ipynb)\n",
    "word2idx = np.load('../00_data/encoded/word2idx.npy').item()\n",
    "idx2word = np.load('../00_data/encoded/idx2word.npy').item()\n",
    "pos2idx = np.load('../00_data/encoded/pos2idx.npy').item()\n",
    "idx2pos = np.load('../00_data/encoded/idx2pos.npy').item()\n",
    "ner2idx = np.load('../00_data/encoded/ner2idx.npy').item()\n",
    "idx2ner = np.load('../00_data/encoded/idx2ner.npy').item()\n",
    "\n",
    "X_tokens = list(np.load('../00_data/encoded/add_tokens.npy'))\n",
    "X_postags = list(np.load('../00_data/encoded/add_postags.npy'))\n",
    "y_nertags = list(np.load('../00_data/encoded/add_nertags.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding data\n",
    "w2v_vocab, _ = load_vocab('../00_data/embeddings/text_mapping.json')\n",
    "w2v_model = Word2Vec.load('../00_data/embeddings/text_embeddings.gensimmodel')\n",
    "w2v_pvocab, _ = load_vocab('../00_data/embeddings/postag_mapping.json')\n",
    "w2v_pmodel = Word2Vec.load('../00_data/embeddings/postag_embeddings.gensimmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change category label here\n",
    "y_speechacts = np.load('../00_data/encoded/add_speechacts.npy')\n",
    "y_topics = np.load('../00_data/encoded/add_topics.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create speechact dictionary\n",
    "def listfromdicts(lst):\n",
    "    setlst = list(set(lst))\n",
    "    x2i = dict(list(zip(setlst, [i for i in range(len(setlst))])))\n",
    "    i2x = dict(list(zip([i for i in range(len(setlst))], setlst)))\n",
    "    return x2i, i2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save dictionaries\n",
    "sa2idx, idx2sa = listfromdicts(y_speechacts)\n",
    "np.save('../00_data/encoded/sa2idx.npy', sa2idx)\n",
    "np.save('../00_data/encoded/idx2sa.npy', idx2sa)\n",
    "top2idx, idx2top = listfromdicts(y_topics)\n",
    "np.save('../00_data/encoded/top2idx.npy', top2idx)\n",
    "np.save('../00_data/encoded/idx2top.npy', idx2top)\n",
    "print(len(idx2sa.keys()), len(top2idx.keys()))\n",
    "# integer-index data\n",
    "X_tokens = index_sents(X_tokens, word2idx)\n",
    "X_postags = index_sents(X_postags, pos2idx)\n",
    "y_nertags = index_sents(y_nertags, ner2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer-index and one-hot speech-acts\n",
    "INT_VOCAB = len(list(idx2sa.keys()))\n",
    "y_speechacts = one_hot([sa2idx[sa] for sa in y_speechacts], dtype='int', num_labels=INT_VOCAB)\n",
    "\n",
    "TOP_VOCAB = len(list(idx2top.keys()))\n",
    "y_topics = one_hot([top2idx[t] for t in y_topics], dtype='int', num_labels=TOP_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "split_idx = int(len(X_tokens)*0.9)\n",
    "X_train_sents = X_tokens[:split_idx]\n",
    "X_train_pos = X_postags[:split_idx]\n",
    "y_train_ner = y_nertags[:split_idx]\n",
    "X_test_sents = X_tokens[split_idx:]\n",
    "X_test_pos = X_postags[split_idx:]\n",
    "y_test_ner = y_nertags[split_idx:]\n",
    "\n",
    "y_train_sacts = y_speechacts[:split_idx]\n",
    "y_test_sacts = y_speechacts[split_idx:]\n",
    "y_train_tops = y_topics[:split_idx]\n",
    "y_test_tops = y_topics[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network hyperparameters\n",
    "MAX_LENGTH      = 20\n",
    "MAX_VOCAB       = len(word2idx.keys()) # see preprocessing\n",
    "EMBEDDING_SIZE  = 160                  # see preprocessing\n",
    "POSBEDDING_SIZE = 32\n",
    "HIDDEN_SIZE     = 192\n",
    "BATCH_SIZE      = 32\n",
    "DROPOUTRATE     = 0.5\n",
    "MAX_EPOCHS      = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of pos-tags, ner tags\n",
    "TAG_VOCAB = len(list(idx2pos.keys()))\n",
    "NER_VOCAB = len(list(idx2ner.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-pad the sequences to max length\n",
    "X_train_sents = sequence.pad_sequences(X_train_sents, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "X_test_sents  = sequence.pad_sequences(X_test_sents, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "X_train_pos   = sequence.pad_sequences(X_train_pos, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "X_test_pos    = sequence.pad_sequences(X_test_pos, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "y_train_ner   = sequence.pad_sequences(y_train_ner, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "y_test_ner    = sequence.pad_sequences(y_test_ner, maxlen=MAX_LENGTH, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for CRF\n",
    "y_train_ner = y_train_ner[:, :, np.newaxis]\n",
    "y_test_ner  = y_test_ner[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding matrices from custom pretrained word2vec embeddings\n",
    "word_embedding_matrix = np.zeros((MAX_VOCAB, EMBEDDING_SIZE))\n",
    "\n",
    "for word in word2idx.keys():\n",
    "    # get the word vector from the embedding model\n",
    "    # if it's there (check against vocab list)\n",
    "    if word in w2v_vocab:\n",
    "        # get the word vector\n",
    "        word_vector = w2v_model[word]\n",
    "        # slot it in at the proper index\n",
    "        word_embedding_matrix[word2idx[word]] = word_vector\n",
    "\n",
    "pos_embedding_matrix = np.zeros((TAG_VOCAB, POSBEDDING_SIZE))\n",
    "\n",
    "for word in pos2idx.keys():\n",
    "    # get the word vector from the embedding model\n",
    "    # if it's there (check against vocab list)\n",
    "    if word in w2v_pvocab:\n",
    "        # get the word vector\n",
    "        word_vector = w2v_pmodel[word]\n",
    "        # slot it in at the proper index\n",
    "        pos_embedding_matrix[pos2idx[word]] = word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "# word layers : dense embedding > dropout > bi-LSTM\n",
    "txt_input = Input(shape=(MAX_LENGTH,), name='txt_input')\n",
    "txt_embed = Embedding(MAX_VOCAB, EMBEDDING_SIZE, input_length=MAX_LENGTH,\n",
    "                      weights=[word_embedding_matrix],\n",
    "                      name='txt_embedding', trainable=False, mask_zero=True)(txt_input)\n",
    "txt_drpot = Dropout(DROPOUTRATE, name='txt_dropout')(txt_embed)\n",
    "txt_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                          name='txt_bidirectional')(txt_drpot)\n",
    "\n",
    "# pos layers : dense embedding > dropout > bi-LSTM\n",
    "pos_input = Input(shape=(MAX_LENGTH,), name='pos_input')\n",
    "pos_embed = Embedding(TAG_VOCAB, POSBEDDING_SIZE, input_length=MAX_LENGTH,\n",
    "                      weights=[pos_embedding_matrix],\n",
    "                      name='pos_embedding', trainable=True, mask_zero=True)(pos_input)\n",
    "pos_drpot = Dropout(DROPOUTRATE, name='pos_dropout')(pos_embed)\n",
    "\n",
    "# merged layers : concat word and pos > bi-LSTM\n",
    "mrg_cncat = concatenate([txt_lstml, pos_drpot], axis=2)\n",
    "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                          name='mrg_bidirectional_1')(mrg_cncat)\n",
    "\n",
    "# final NER linear chain CRF layer\n",
    "crf = CRF(NER_VOCAB, sparse_target=True)\n",
    "out_ner = crf(mrg_lstml)\n",
    "\n",
    "# intent network\n",
    "rnn_intent = Attention(name='int_attention')(mrg_lstml)\n",
    "\n",
    "# intent\n",
    "dns_intent = Dense(INT_VOCAB, activation='relu', name='int_dense_1')(rnn_intent)\n",
    "dns_intent = Dense(INT_VOCAB, name='int_dense_2')(dns_intent)\n",
    "out_intent = Activation('softmax', name='int_output')(dns_intent)\n",
    "\n",
    "# topic\n",
    "dns_top = Dense(TOP_VOCAB, activation='relu', name='top_dense_1')(rnn_intent)\n",
    "dns_intent = Dense(INT_VOCAB, name='int_dense_2')(dns_top)\n",
    "out_top = Activation('softmax', name='top_output')(dns_top)\n",
    "\n",
    "model = Model(inputs=[txt_input, pos_input], outputs=[out_ner, out_intent, out_top])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss={'crf_1': crf.loss_function, 'int_output': 'categorical_crossentropy',  'top_output': 'categorical_crossentropy'},\n",
    "              loss_weights={'crf_1': 0.25, 'int_output': 0.5, 'top_output': 0.75},\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_train_sents, X_train_pos], [y_train_ner, y_train_sacts, y_train_tops],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    callbacks=[TQDMNotebookCallback()],\n",
    "                    verbose=0)\n",
    "\n",
    "hist_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# because we are using keras-contrib, we must save weights like this, and load into network\n",
    "save_load_utils.save_all_weights(model, '../00_data/model/alt_combo_model.h5')\n",
    "np.save('../00_data/model/alt_combo_dict.npy', hist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate([X_test_sents, X_test_pos], [y_test_ner, y_test_sacts, y_test_tops])\n",
    "print('')\n",
    "print('Eval model...')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
