{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import Activation, concatenate, Dense, Input, LSTM, Dropout, Embedding\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from gensim.models import Word2Vec\n",
    "from mlxtend.preprocessing import one_hot\n",
    "from embedding import load_vocab\n",
    "from attention.attention import Attention\n",
    "from mltools.preprocessing import Tokenizer, Indexer, Pipeline, LabelIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict GPU usage here\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  = np.load('../00_data/encoded/snips_x_train.npy')\n",
    "x_test   = np.load('../00_data/encoded/snips_x_test.npy')\n",
    "yt_train = np.load('../00_data/encoded/snips_y_tags_train.npy')\n",
    "yt_test  = np.load('../00_data/encoded/snips_y_tags_test.npy')\n",
    "yi_train = np.load('../00_data/encoded/snips_y_int_train.npy')\n",
    "yi_test  = np.load('../00_data/encoded/snips_y_int_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_indexer = pickle.load(open(\"../00_data/encoded/snips_intent_indexer.pkl\", \"rb\"))\n",
    "label_indexer  = pickle.load(open(\"../00_data/encoded/snips_label_indexer.pkl\", \"rb\"))\n",
    "word_idxpipe   = pickle.load(open(\"../00_data/encoded/snips_sent_indexer.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding data\n",
    "w2v_vocab, _ = load_vocab('../00_data/embeddings/snips_mapping.json')\n",
    "w2v_model = Word2Vec.load('../00_data/embeddings/snips_embeddings.gensimmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training data, for validation_size\n",
    "shuffle_idx = np.random.permutation(x_train.shape[0])\n",
    "\n",
    "x_train  = x_train[shuffle_idx]\n",
    "yt_train = yt_train[shuffle_idx]\n",
    "yi_train = yi_train[shuffle_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network hyperparameters\n",
    "MAX_LENGTH      = 15    # see preprocessing\n",
    "MAX_VOCAB       = 10000 # see preprocessing\n",
    "EMBEDDING_SIZE  = 300   # see preprocessing\n",
    "HIDDEN_SIZE     = 300\n",
    "DROPOUTRATE     = 0.50\n",
    "BATCH_SIZE      = 128\n",
    "MAX_EPOCHS      = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of the intent, tag vocab\n",
    "INT_VOCAB = len(list(intent_indexer.idx2tag.keys()))\n",
    "TAG_VOCAB = len(list(label_indexer.idx2tag.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load embeddings from the trained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 9998 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrices from custom pretrained word2vec embeddings\n",
    "word_embedding_matrix = np.zeros((MAX_VOCAB, EMBEDDING_SIZE))\n",
    "c = 0\n",
    "for word in word_idxpipe.steps[1][1].word2idx.keys():\n",
    "    # get the word vector from the embedding model\n",
    "    # if it's there (check against vocab list)\n",
    "    if word in w2v_vocab:\n",
    "        c += 1\n",
    "        # get the word vector\n",
    "        word_vector = w2v_model[word]\n",
    "        # slot it in at the proper index\n",
    "        word_embedding_matrix[word_idxpipe.steps[1][1].word2idx[word]] = word_vector\n",
    "print('added', c, 'embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "# word input layers : dense embedding > dropout\n",
    "txt_input = Input(shape=(MAX_LENGTH,), name='txt_input')\n",
    "txt_embed = Embedding(MAX_VOCAB, EMBEDDING_SIZE, input_length=MAX_LENGTH,\n",
    "                      weights=[word_embedding_matrix],\n",
    "                      name='txt_embedding', trainable=True, mask_zero=True)(txt_input)\n",
    "txt_drpot = Dropout(DROPOUTRATE, name='txt_dropout')(txt_embed)\n",
    "\n",
    "# recurrent layers : bi-LSTM\n",
    "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                          name='bidirectional_1')(txt_drpot)\n",
    "mrg_lstml = Dropout(DROPOUTRATE, name='bidirectional_drop')(mrg_lstml)\n",
    "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                          name='bidirectional_2')(mrg_lstml)\n",
    "\n",
    "# final NER linear chain CRF layer\n",
    "crf = CRF(TAG_VOCAB, sparse_target=True, name='crf_1')\n",
    "out_ner = crf(mrg_lstml)\n",
    "\n",
    "# intent network\n",
    "rnn_intent = Attention(name='int_attention')(mrg_lstml)\n",
    "\n",
    "# intent\n",
    "dns_intent = Dense(INT_VOCAB, activation='relu', name='int_dense_1')(rnn_intent)\n",
    "dns_intent = Dense(INT_VOCAB, name='int_dense_2')(dns_intent)\n",
    "out_intent = Activation('softmax', name='int_output')(dns_intent)\n",
    "\n",
    "model = Model(inputs=txt_input, outputs=[out_ner, out_intent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "txt_input (InputLayer)          (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "txt_embedding (Embedding)       (None, 15, 300)      3000000     txt_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "txt_dropout (Dropout)           (None, 15, 300)      0           txt_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 15, 600)      1442400     txt_dropout[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_drop (Dropout)    (None, 15, 600)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 15, 600)      2162400     bidirectional_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "int_attention (Attention)       (None, 600)          361200      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "int_dense_1 (Dense)             (None, 7)            4207        int_attention[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "int_dense_2 (Dense)             (None, 7)            56          int_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 15, 41)       26404       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "int_output (Activation)         (None, 7)            0           int_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,996,667\n",
      "Trainable params: 6,996,667\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss={'crf_1': crf.loss_function, 'int_output': 'sparse_categorical_crossentropy'},\n",
    "              loss_weights={'crf_1': 0.5, 'int_output': 0.5},\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13784, 15), (13784, 15, 1), (13784, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, yt_train.shape, yi_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False]), array([False]), array([False]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.isnan(x_train)), np.unique(np.isnan(yt_train)), np.unique(np.isnan(yi_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9999, 9999)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_VOCAB, np.max(x_train), np.max(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 40, 40)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAG_VOCAB, np.max(yt_train), np.max(yt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 6, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INT_VOCAB, np.max(yi_train), np.max(yi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " - 14s - loss: 4.0758 - crf_1_loss: 6.2224 - int_output_loss: 1.9292\n",
      "Epoch 2/25\n",
      " - 11s - loss: 3.7249 - crf_1_loss: 5.5619 - int_output_loss: 1.8879\n",
      "Epoch 3/25\n",
      " - 11s - loss: 3.6368 - crf_1_loss: 5.4284 - int_output_loss: 1.8452\n",
      "Epoch 4/25\n",
      " - 11s - loss: 3.5740 - crf_1_loss: 5.3472 - int_output_loss: 1.8009\n",
      "Epoch 5/25\n",
      " - 12s - loss: 3.5181 - crf_1_loss: 5.2843 - int_output_loss: 1.7520\n",
      "Epoch 6/25\n",
      " - 11s - loss: 3.4662 - crf_1_loss: 5.2339 - int_output_loss: 1.6986\n",
      "Epoch 7/25\n",
      " - 11s - loss: 3.4134 - crf_1_loss: 5.1897 - int_output_loss: 1.6371\n",
      "Epoch 8/25\n",
      " - 11s - loss: 3.3603 - crf_1_loss: 5.1485 - int_output_loss: 1.5721\n",
      "Epoch 9/25\n",
      " - 12s - loss: 3.3053 - crf_1_loss: 5.1103 - int_output_loss: 1.5002\n",
      "Epoch 10/25\n",
      " - 11s - loss: 3.2476 - crf_1_loss: 5.0719 - int_output_loss: 1.4232\n",
      "Epoch 11/25\n",
      " - 11s - loss: 3.1887 - crf_1_loss: 5.0346 - int_output_loss: 1.3427\n",
      "Epoch 12/25\n",
      " - 11s - loss: 3.1278 - crf_1_loss: 4.9979 - int_output_loss: 1.2577\n",
      "Epoch 13/25\n",
      " - 12s - loss: 3.0664 - crf_1_loss: 4.9609 - int_output_loss: 1.1719\n",
      "Epoch 14/25\n",
      " - 12s - loss: 3.0066 - crf_1_loss: 4.9260 - int_output_loss: 1.0871\n",
      "Epoch 15/25\n",
      " - 12s - loss: 2.9471 - crf_1_loss: 4.8916 - int_output_loss: 1.0025\n",
      "Epoch 16/25\n",
      " - 11s - loss: 2.8898 - crf_1_loss: 4.8593 - int_output_loss: 0.9203\n",
      "Epoch 17/25\n",
      " - 11s - loss: 2.8347 - crf_1_loss: 4.8270 - int_output_loss: 0.8423\n",
      "Epoch 18/25\n",
      " - 11s - loss: 2.7837 - crf_1_loss: 4.7967 - int_output_loss: 0.7707\n",
      "Epoch 19/25\n",
      " - 11s - loss: 2.7386 - crf_1_loss: 4.7676 - int_output_loss: 0.7095\n",
      "Epoch 20/25\n",
      " - 11s - loss: 2.6992 - crf_1_loss: 4.7396 - int_output_loss: 0.6588\n",
      "Epoch 21/25\n",
      " - 11s - loss: 2.6665 - crf_1_loss: 4.7129 - int_output_loss: 0.6201\n",
      "Epoch 22/25\n",
      " - 11s - loss: 2.6380 - crf_1_loss: 4.6882 - int_output_loss: 0.5878\n",
      "Epoch 23/25\n",
      " - 11s - loss: 2.6120 - crf_1_loss: 4.6640 - int_output_loss: 0.5599\n",
      "Epoch 24/25\n",
      " - 11s - loss: 2.5897 - crf_1_loss: 4.6421 - int_output_loss: 0.5373\n",
      "Epoch 25/25\n",
      " - 11s - loss: 2.5716 - crf_1_loss: 4.6215 - int_output_loss: 0.5216\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([x_train], [yt_train, yi_train],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    verbose=2)\n",
    "\n",
    "hist_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# because we are using keras-contrib, we must save weights like this, and load into network\n",
    "save_load_utils.save_all_weights(model, '../00_data/model/snips_combo_model.h5')\n",
    "np.save('../00_data/model/snips_combo_dict.npy', hist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 1s 2ms/step\n",
      "\n",
      "Eval model...\n",
      "[2.8431013679504393, 4.859860506057739, 0.8263422076616969]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, [yt_test, yi_test])\n",
    "print('')\n",
    "print('Eval model...')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags, test_intents = model.predict(x_test)\n",
    "np.save('../00_data/model/snips_pred_tags.npy', test_tags)\n",
    "np.save('../00_data/model/snips_pred_ints.npy', test_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
