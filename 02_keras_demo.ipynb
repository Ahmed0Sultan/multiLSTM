{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoding demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/derek/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/derek/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from preprocessing import CharacterIndexer, SlotIndexer, IntentIndexer\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/derek/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import math\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Concatenate, concatenate, Dense, Dropout, Embedding, Input, TimeDistributed\n",
    "from keras.layers import LSTM, CuDNNLSTM, LeakyReLU, Masking, Lambda, Dot, BatchNormalization, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TerminateOnNaN, ModelCheckpoint\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from kutilities.layers import AttentionWithContext\n",
    "from attention import TDAttention\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Activation, Multiply, Add, Lambda\n",
    "import keras.initializers\n",
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentindexer = pickle.load(open('encoded/atis_sentindexer.pkl', 'rb'))\n",
    "slotindexer = pickle.load(open('encoded/atis_slotindexer.pkl', 'rb'))\n",
    "intindexer  = pickle.load(open('encoded/atis_intindexer.pkl',  'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model loading\n",
    "\n",
    "due to the `keras-contrib` CRF and added attention layers, the easiest way to load the model is to recreate it and load the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728 22 121 22\n"
     ]
    }
   ],
   "source": [
    "# preprocessing-dependent parameters\n",
    "# we can use the indexer attributes\n",
    "TXT_VOCAB  = sentindexer.max_word_vocab\n",
    "TXT_MAXLEN = sentindexer.max_sent_len\n",
    "CHR_MAXLEN = sentindexer.max_word_len\n",
    "CHR_VOCAB  = sentindexer.max_char_vocab\n",
    "SLOT_NUM   = slotindexer.labelsize\n",
    "LABEL_NUM  = intindexer.labelsize\n",
    "print(TXT_VOCAB, TXT_MAXLEN, SLOT_NUM, LABEL_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined network hyperparameters\n",
    "WEMBED_SIZE   = 200   # word embedding size. must match w2v size\n",
    "CEMBED_SIZE   = 200   # character embedding size. free param\n",
    "WDROP_RATE    = 0.50  # word-level input dropout\n",
    "DROP_RATE     = 0.33  # dropout for other layers\n",
    "RNN_DROP_RATE = 0.0   # recurrent droput (not implemented)\n",
    "HIDDEN_SIZE   = 300   # LSTM block hidden size\n",
    "BATCH_SIZE    = 32\n",
    "MAX_EPOCHS    = 50\n",
    "OPTIMIZER     = keras.optimizers.Adadelta(clipnorm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highway(inputs, activation=\"tanh\", gate_bias=-2):\n",
    "    feats = K.int_shape(inputs)[-1]\n",
    "    gate_bias_init = keras.initializers.Constant(gate_bias)\n",
    "    transform_gate = Dense(units=feats, bias_initializer=gate_bias_init, activation='sigmoid')(inputs)\n",
    "    carry_gate = Lambda(lambda x: 1.0 - x, output_shape=(feats,))(transform_gate)\n",
    "    h_transformed = Dense(units=feats)(inputs)\n",
    "    h_transformed = Activation(activation)(h_transformed)\n",
    "    transformed_gated = Multiply()([transform_gate, h_transformed])\n",
    "    carried_gated = Multiply()([carry_gate, inputs])\n",
    "    outputs = Add()([transformed_gated, carried_gated])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Kim; Ma & Hovy char-CNN + word input\n",
    "########################################\n",
    "\n",
    "# word-level input with word embedding matrix (with word2vec)\n",
    "txt_input = Input(shape=(TXT_MAXLEN,), name='word_input')\n",
    "\n",
    "txt_embed = Embedding(TXT_VOCAB, WEMBED_SIZE, input_length=TXT_MAXLEN,\n",
    "                      name='word_embedding', trainable=True, mask_zero=True)(txt_input)\n",
    "\n",
    "txt_drpot = Dropout(WDROP_RATE, name='word_dropout')(txt_embed)\n",
    "\n",
    "# character-level input with randomized initializations\n",
    "cnn_input = Input(shape=(TXT_MAXLEN, CHR_MAXLEN), name='cnn_input')\n",
    "\n",
    "cnn_embed = TimeDistributed(Embedding(CHR_VOCAB, CEMBED_SIZE, input_length=CHR_MAXLEN,\n",
    "                            name='cnn_embedding', trainable=True, mask_zero=False))(cnn_input)\n",
    "\n",
    "# 1-size window CNN with batch-norm & tanh activation (Kim 2015)\n",
    "cnns1 = TimeDistributed(Conv1D(filters=10, kernel_size=1, padding=\"same\", strides=1), name='cnn1_cnn')(cnn_embed)\n",
    "cnns1 = TimeDistributed(BatchNormalization(), name='cnn1_bnorm')(cnns1)\n",
    "cnns1 = TimeDistributed(Activation('tanh'), name='cnn1_act')(cnns1)\n",
    "cnns1 = TimeDistributed(GlobalMaxPooling1D(), name='cnn1_gmp')(cnns1)\n",
    "\n",
    "# 2-size window CNN with batch-norm & tanh activation (Kim 2015)\n",
    "cnns2 = TimeDistributed(Conv1D(filters=20, kernel_size=2, padding=\"same\", strides=1), name='cnn2_cnn')(cnn_embed)\n",
    "cnns2 = TimeDistributed(BatchNormalization(), name='cnn2_bnorm')(cnns2)\n",
    "cnns2 = TimeDistributed(Activation('tanh'), name='cnn2_act')(cnns2)\n",
    "cnns2 = TimeDistributed(GlobalMaxPooling1D(), name='cnn2_gmp')(cnns2)\n",
    "\n",
    "# 3-size window CNN with batch-norm & tanh activation (Kim 2015)\n",
    "cnns3 = TimeDistributed(Conv1D(filters=30, kernel_size=3, padding=\"same\", strides=1), name='cnn3_cnn')(cnn_embed)\n",
    "cnns3 = TimeDistributed(BatchNormalization(), name='cnn3_bnorm')(cnns3)\n",
    "cnns3 = TimeDistributed(Activation('tanh'), name='cnn3_act')(cnns3)\n",
    "cnns3 = TimeDistributed(GlobalMaxPooling1D(), name='cnn3_gmp')(cnns3)\n",
    "\n",
    "# 4-size window CNN with batch-norm & tanh activation (Kim 2015)\n",
    "cnns4 = TimeDistributed(Conv1D(filters=40, kernel_size=4, padding=\"same\", strides=1), name='cnn4_cnn')(cnn_embed)\n",
    "cnns4 = TimeDistributed(BatchNormalization(), name='cnn4_bnorm')(cnns4)\n",
    "cnns4 = TimeDistributed(Activation('tanh'), name='cnn4_act')(cnns4)\n",
    "cnns4 = TimeDistributed(GlobalMaxPooling1D(), name='cnn4_gmp')(cnns4)\n",
    "\n",
    "# time-distributed highway layer (Kim 2015)\n",
    "cnns  = concatenate([cnns1, cnns2, cnns3, cnns4], axis=-1, name='cnn_concat')\n",
    "cnns  = TimeDistributed(Lambda(highway), name='cnn_highway')(cnns)\n",
    "\n",
    "# final concat of convolutional subword embeddings and word vectors\n",
    "word_vects  = concatenate([cnns, txt_drpot], axis=-1, name='concat_word_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# main recurrent sentence block\n",
    "########################################\n",
    "\n",
    "# 'encoder' layer with returned states following (Liu, Lane)\n",
    "lstm_enc, fh, fc, bh, bc  = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True),\n",
    "                                          name='bidirectional_enc')(word_vects)\n",
    "lstm_enc = Dropout(DROP_RATE, name='bidirectional_dropout_enc')(lstm_enc)\n",
    "\n",
    "# \"aligned seq2seq\" lstm\n",
    "# load forward LSTM with reverse states following Liu, Lane 2016 (and do reverse)\n",
    "lstm_dec = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                         name='bidirectional_dec')(lstm_enc, initial_state=[bh, bc, fh, fc])\n",
    "\n",
    "lstm_states = Dropout(DROP_RATE, name='bidirectional_dropout_dec')(lstm_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Huang et al; Ma & Hovy CRF slot clf\n",
    "########################################\n",
    "\n",
    "# final slot linear chain CRF layer\n",
    "lyr_crf   = CRF(SLOT_NUM, sparse_target=True, name='out_slot', learn_mode='marginal', test_mode='marginal')\n",
    "out_slot  = lyr_crf(lstm_states)\n",
    "\n",
    "# alternative is using greedy predictions\n",
    "# out_slot  = TimeDistributed(Dense(SLOT_NUM, activation='softmax'), name='out_slot')(txt_lstm_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# attentional intent clf block\n",
    "########################################\n",
    "\n",
    "# combine lstm with CRF for attention (see Liu & Lane)\n",
    "seq_concat = concatenate([lstm_states, out_slot], axis=2, name='lstm_concat')\n",
    "seq_concat = Dropout(DROP_RATE, name='bidirectional_dropout_3')(seq_concat)\n",
    "\n",
    "# layer: intent attention w/context (Liu & Lane)\n",
    "att_int = AttentionWithContext(name='intent_attention')(seq_concat)\n",
    "\n",
    "# layer: dense + LeakyReLU with dropout\n",
    "out_int = Dense(K.int_shape(att_int)[-1],\n",
    "                kernel_regularizer=l2(0.005),\n",
    "                name='intent_dense_1')(att_int)\n",
    "out_int = LeakyReLU(name='intent_act_1')(out_int)\n",
    "out_int = Dropout(DROP_RATE, name='intent_dropout_1')(out_int)\n",
    "\n",
    "# layer: dense + LeakyReLU with dropout\n",
    "out_int = Dense(K.int_shape(att_int)[-1],\n",
    "                kernel_regularizer=l2(0.0025),\n",
    "                name='intent_dense_2')(out_int)\n",
    "out_int = LeakyReLU(name='intent_act_2')(out_int)\n",
    "\n",
    "# layer: final dense + softmax\n",
    "out_int = Dense(LABEL_NUM, activation='softmax', name='out_intent')(out_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[txt_input, cnn_input], outputs=[out_slot, out_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'test_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test\n",
    "model.load_weights('model/'+modelname+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=OPTIMIZER,\n",
    "              loss={'out_slot': lyr_crf.loss_function, 'out_intent': 'sparse_categorical_crossentropy'},\n",
    "              # loss={'out_slot': 'sparse_categorical_crossentropy', 'out_intent': 'sparse_categorical_crossentropy'},\n",
    "              loss_weights={'out_slot': 0.5, 'out_intent': 0.5},\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoding functions\n",
    "\n",
    "format the input string properly (lower-case, add BOS and EOS tags, strip punctuation, and index), predict on the model, use `argmax` to get predictions then `inverse_transform()` back into human-readable labels\n",
    "\n",
    "a more convenient way to do this would be to encapsulate all the above and below into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(snt):\n",
    "    snt = snt.lower()\n",
    "    snt = re.sub(r'[^0-9a-z\\s]', '', snt)\n",
    "    snt = snt.split()\n",
    "    snt = ['BOS'] + snt + ['EOS']\n",
    "    snt = [snt]\n",
    "    out = sentindexer.transform(snt)\n",
    "    return snt, out[0], out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(s):\n",
    "    tk, wt, ct = preprocess(s)\n",
    "    tk = tk[0]\n",
    "    sp, ip = model.predict([wt, ct])\n",
    "    sp = np.argmax(sp, axis=-1)\n",
    "    ip = np.argmax(ip, axis=-1)\n",
    "    sp = slotindexer.inverse_transform(np.expand_dims(sp, axis=-1))[0]\n",
    "    sp = [x.split('-')[-1] for x in sp]\n",
    "    \n",
    "    spd = {}\n",
    "    for i, p in enumerate(sp):\n",
    "        if p != 'O':\n",
    "            if p in spd.keys():\n",
    "                spd[p].append(tk[i])\n",
    "            else:\n",
    "                spd[p] = []\n",
    "                spd[p].append(tk[i])\n",
    "    \n",
    "    spo = {}\n",
    "    for k in spd.keys():\n",
    "        spo[k] = ' '.join(spd[k])\n",
    "    \n",
    "    ip = intindexer.inverse_transform([ip]+[[0]])[0]\n",
    "\n",
    "    print('query:', s)\n",
    "    print('slots:')\n",
    "    print(spo)\n",
    "    print('intent:', ip)\n",
    "    \n",
    "    return spo, ip\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: looking for direct flights from Chicago to LAX\n",
      "slots:\n",
      "{'connect': 'direct', 'fromloc.city_name': 'chicago', 'toloc.city_name': 'lax'}\n",
      "intent: atis_flight\n"
     ]
    }
   ],
   "source": [
    "inpt = \"looking for direct flights from Chicago to LAX\"\n",
    "a, b = predict(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: give me flights and fares from New York to Dallas\n",
      "slots:\n",
      "{'fromloc.city_name': 'new york', 'toloc.city_name': 'dallas'}\n",
      "intent: atis_flight#atis_airfare\n"
     ]
    }
   ],
   "source": [
    "inpt = \"give me flights and fares from New York to Dallas\"\n",
    "a, b = predict(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
