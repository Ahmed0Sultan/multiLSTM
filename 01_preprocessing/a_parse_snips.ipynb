{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, train_tags, train_intents = [], [], []\n",
    "path = '../00_data/snips/train'\n",
    "for filename in os.listdir(path):\n",
    "    with open(path + '/' + filename) as json_file:\n",
    "        intent = filename.split('_')[1]\n",
    "        try:\n",
    "            data = json.load(json_file)\n",
    "            data = data[intent]\n",
    "            # print(data[:5])\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        for sent in data:\n",
    "            s, t = [], []\n",
    "            for dct in sent['data']:\n",
    "                if 'entity' in dct.keys():\n",
    "                    t.append(dct['entity'])\n",
    "                    s.append(dct['text'])\n",
    "                else:\n",
    "                    t.append(\"NONE\")\n",
    "                    s.append(dct['text'])\n",
    "            train_sents.append(s)\n",
    "            train_tags.append(t)\n",
    "            train_intents.append(intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13784, 13784, 13784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sents), len(train_tags), len(train_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GetWeather', 2000),\n",
       " ('PlayMusic', 2000),\n",
       " ('BookRestaurant', 1973),\n",
       " ('SearchScreeningEvent', 1959),\n",
       " ('RateBook', 1956),\n",
       " ('SearchCreativeWork', 1954),\n",
       " ('AddToPlaylist', 1942)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_intents).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchCreativeWork\n",
      "GetWeather\n",
      "PlayMusic\n",
      "RateBook\n",
      "SearchScreeningEvent\n",
      "BookRestaurant\n",
      "AddToPlaylist\n"
     ]
    }
   ],
   "source": [
    "val_sents, val_tags, val_intents = [], [], []\n",
    "path = '../00_data/snips/validate'\n",
    "for filename in os.listdir(path):\n",
    "    if 'json' in filename:\n",
    "        with open(path + '/' + filename) as json_file:\n",
    "            intent = filename.split('_')[1]\n",
    "            intent = intent.split('.')[0]\n",
    "            print(intent)\n",
    "            try:\n",
    "                data = json.load(json_file)\n",
    "                data = data[intent]\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            for sent in data:\n",
    "                s, t = [], []\n",
    "                for dct in sent['data']:\n",
    "                    if 'entity' in dct.keys():\n",
    "                        t.append(dct['entity'])\n",
    "                        s.append(dct['text'])\n",
    "                    else:\n",
    "                        t.append(\"NONE\")\n",
    "                        s.append(dct['text'])\n",
    "                val_sents.append(s)\n",
    "                val_tags.append(t)\n",
    "                val_intents.append(intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 700, 700)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_sents), len(val_tags), len(val_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SearchCreativeWork', 100),\n",
       " ('GetWeather', 100),\n",
       " ('PlayMusic', 100),\n",
       " ('RateBook', 100),\n",
       " ('SearchScreeningEvent', 100),\n",
       " ('BookRestaurant', 100),\n",
       " ('AddToPlaylist', 100)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(val_intents).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Add another ', 'song', ' to the ', 'Cita Romántica', ' playlist. ']\n",
      "['NONE', 'music_item', 'NONE', 'playlist', 'NONE']\n",
      "AddToPlaylist\n"
     ]
    }
   ],
   "source": [
    "print(train_sents[0])\n",
    "print(train_tags[0])\n",
    "print(train_intents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess sentences\n",
    "def cleanup(sentlist, taglist):\n",
    "    newsents = []\n",
    "    newtags = []\n",
    "    for idx, ss in enumerate(sentlist):\n",
    "        nss, ntt = [], []\n",
    "        for jdx, s in  enumerate(ss):\n",
    "            s = s.lower()\n",
    "            for c in ['.', ',', '!', '?', ]:\n",
    "                s = s.replace(c, '')\n",
    "            tt = s.split()\n",
    "            for t in tt:\n",
    "                nss.append(t)\n",
    "                ntt.append(taglist[idx][jdx])\n",
    "        newsents.append(nss)\n",
    "        newtags.append(ntt)\n",
    "    return newsents, newtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_c, train_tags_c = cleanup(train_sents, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sents_c, val_tags_c = cleanup(val_sents, val_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddToPlaylist\n",
      "['add', 'another', 'song', 'to', 'the', 'cita', 'romántica', 'playlist']\n",
      "['NONE', 'NONE', 'music_item', 'NONE', 'NONE', 'playlist', 'playlist', 'NONE']\n",
      "\n",
      "AddToPlaylist\n",
      "['add', 'clem', 'burke', 'in', 'my', 'playlist', 'pre-party', 'r&b', 'jams']\n",
      "['NONE', 'artist', 'artist', 'NONE', 'playlist_owner', 'NONE', 'playlist', 'playlist', 'playlist']\n",
      "\n",
      "AddToPlaylist\n",
      "['add', 'live', 'from', 'aragon', 'ballroom', 'to', 'trapeo']\n",
      "['NONE', 'entity_name', 'entity_name', 'entity_name', 'entity_name', 'NONE', 'playlist']\n",
      "\n",
      "AddToPlaylist\n",
      "['add', 'unite', 'and', 'win', 'to', 'my', 'night', 'out']\n",
      "['NONE', 'entity_name', 'entity_name', 'entity_name', 'NONE', 'playlist_owner', 'playlist', 'playlist']\n",
      "\n",
      "AddToPlaylist\n",
      "['add', 'track', 'to', 'my', 'digster', 'future', 'hits']\n",
      "['NONE', 'music_item', 'NONE', 'playlist_owner', 'playlist', 'playlist', 'playlist']\n",
      "\n",
      "AddToPlaylist\n",
      "['add', 'the', 'piano', 'bar', 'to', 'my', 'cindy', 'wilson']\n",
      "['NONE', 'playlist', 'playlist', 'playlist', 'NONE', 'playlist_owner', 'artist', 'artist']\n",
      "\n",
      "AddToPlaylist\n",
      "['add', 'spanish', 'harlem', 'incident', 'to', 'cleaning', 'the', 'house']\n",
      "['NONE', 'entity_name', 'entity_name', 'entity_name', 'NONE', 'playlist', 'playlist', 'playlist']\n",
      "\n",
      "AddToPlaylist\n",
      "['add', 'the', 'greyest', 'of', 'blue', 'skies', 'in', 'indie', 'español', 'my', 'playlist']\n",
      "['NONE', 'entity_name', 'entity_name', 'entity_name', 'entity_name', 'entity_name', 'NONE', 'playlist', 'playlist', 'playlist_owner', 'NONE']\n",
      "\n",
      "AddToPlaylist\n",
      "['add', 'the', 'name', 'kids', 'in', 'the', 'street', 'to', 'the', 'plylist', 'new', 'indie', 'mix']\n",
      "['NONE', 'NONE', 'NONE', 'entity_name', 'entity_name', 'entity_name', 'entity_name', 'NONE', 'NONE', 'NONE', 'playlist', 'playlist', 'playlist']\n",
      "\n",
      "AddToPlaylist\n",
      "['add', 'album', 'radar', 'latino']\n",
      "['NONE', 'music_item', 'playlist', 'playlist']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_intents[i])\n",
    "    print(train_sents_c[i])\n",
    "    print(train_tags_c[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchCreativeWork\n",
      "['wish', 'to', 'find', 'the', 'movie', 'the', 'heart', 'beat']\n",
      "['NONE', 'NONE', 'NONE', 'NONE', 'object_type', 'NONE', 'object_name', 'object_name']\n",
      "\n",
      "SearchCreativeWork\n",
      "['please', 'look', 'up', 'the', 'tv', 'show', 'vanity']\n",
      "['NONE', 'NONE', 'NONE', 'NONE', 'object_type', 'object_type', 'object_name']\n",
      "\n",
      "SearchCreativeWork\n",
      "['get', 'me', 'the', \"elvis'\", 'christmas', 'album', 'tv', 'show']\n",
      "['NONE', 'NONE', 'NONE', 'object_name', 'object_name', 'object_name', 'object_type', 'object_type']\n",
      "\n",
      "SearchCreativeWork\n",
      "['please', 'find', 'me', 'the', 'saga', 'the', 'deep', 'six']\n",
      "['NONE', 'NONE', 'NONE', 'NONE', 'object_type', 'object_name', 'object_name', 'object_name']\n",
      "\n",
      "SearchCreativeWork\n",
      "['wish', 'to', 'see', 'the', 'photograph', 'with', 'the', 'name', 'live:', 'right', 'here']\n",
      "['NONE', 'NONE', 'NONE', 'NONE', 'object_type', 'NONE', 'NONE', 'NONE', 'object_name', 'object_name', 'object_name']\n",
      "\n",
      "SearchCreativeWork\n",
      "['looking', 'for', 'a', 'novel', 'called', 'death', 'march']\n",
      "['NONE', 'NONE', 'NONE', 'object_type', 'NONE', 'object_name', 'object_name']\n",
      "\n",
      "SearchCreativeWork\n",
      "['can', 'you', 'find', 'me', 'the', 'work', 'the', 'curse', 'of', 'oak', 'island']\n",
      "['NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'object_name', 'object_name', 'object_name', 'object_name', 'object_name']\n",
      "\n",
      "SearchCreativeWork\n",
      "['please', 'get', 'me', 'the', 'sacred', 'and', 'profane', 'love', 'machine', 'game']\n",
      "['NONE', 'NONE', 'NONE', 'object_name', 'object_name', 'object_name', 'object_name', 'object_name', 'object_name', 'object_type']\n",
      "\n",
      "SearchCreativeWork\n",
      "['need', 'a', 'creative', 'work', 'called', 'hit', 'by', 'love']\n",
      "['NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'object_name', 'object_name', 'object_name']\n",
      "\n",
      "SearchCreativeWork\n",
      "['search', 'for', 'the', 'trailer', 'for', 'the', 'office']\n",
      "['NONE', 'NONE', 'NONE', 'object_type', 'NONE', 'object_name', 'object_name']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(val_intents[:10])):\n",
    "    print(val_intents[i])\n",
    "    print(val_sents_c[i])\n",
    "    print(val_tags_c[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700,), (700,), (700,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(val_intents), np.shape(val_sents_c), np.shape(val_tags_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_sents_c, open('../00_data/snips/train_sents.pkl', 'wb'))\n",
    "pickle.dump(train_tags_c, open('../00_data/snips/train_tags.pkl', 'wb'))\n",
    "pickle.dump(train_intents, open('../00_data/snips/train_intents.pkl', 'wb'))\n",
    "pickle.dump(val_sents_c, open('../00_data/snips/val_sents.pkl', 'wb'))\n",
    "pickle.dump(val_tags_c, open('../00_data/snips/val_tags.pkl', 'wb'))\n",
    "pickle.dump(val_intents, open('../00_data/snips/val_intents.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
