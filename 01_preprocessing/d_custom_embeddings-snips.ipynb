{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create custom word2vec embeddings\n",
    "\n",
    "use a mix of in-domain data (= the merged corpus sentences) and general domain data from the Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataset import get_vocab, index_sents\n",
    "from embedding import create_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in-domain text - lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in in-domain text, POS-tags\n",
    "alltoks = pickle.load(open('../00_data/snips/train_sents.pkl', 'rb'))\n",
    "alltags = pickle.load(open('../00_data/snips/train_tags.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_toks = []\n",
    "for seq in alltoks:\n",
    "    seq = [str(s) for s in seq]\n",
    "    if len(seq) > 1:\n",
    "        string_toks.append(' '.join(seq).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_tags = []\n",
    "for seq in alltags:\n",
    "    seq = [str(s) for s in seq]\n",
    "    if len(seq) > 1:\n",
    "        string_tags.append(' '.join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supplement with generic text - lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12403"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brownsents = brown.sents(categories=['news', 'editorial', 'reviews', 'government'])\n",
    "brownsents = [' '.join(s).lower() for s in brownsents]\n",
    "brownsents = [s.replace(',', '').replace('!', '').replace('?', '').replace('.', '').replace(':', '') for s in brownsents]\n",
    "len(brownsents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_text = brownsents + string_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28618,\n",
       " [('the', 25371),\n",
       "  ('of', 11811),\n",
       "  ('in', 10146),\n",
       "  ('to', 9654),\n",
       "  ('a', 9418),\n",
       "  ('and', 7607),\n",
       "  ('for', 6266),\n",
       "  ('is', 4993),\n",
       "  ('at', 3679),\n",
       "  ('be', 3271)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lists = [s.split() for s in sentence_text]\n",
    "vocab = [w for s in sent_lists for w in s]\n",
    "from collections import Counter\n",
    "vset = set(vocab)\n",
    "vcount = Counter(vocab)\n",
    "len(vset), vcount.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings\n",
    "\n",
    "with open('../00_data/embeddings/snips_text.txt', 'w') as f:\n",
    "    for s in sentence_text:\n",
    "        f.write(s)\n",
    "        f.write('\\n')\n",
    "\n",
    "w2v_vocab, w2v_model = create_embeddings('../00_data/embeddings/snips_text.txt',\n",
    "                       embeddings_path='../00_data/embeddings/snips_embeddings.gensimmodel',\n",
    "                       vocab_path='../00_data/embeddings/snips_mapping.json',\n",
    "                       min_count=1,\n",
    "                       workers=4,\n",
    "                       size = 300,\n",
    "                       iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('strays', 0.9344868659973145),\n",
       " ('ismol', 0.9261569976806641),\n",
       " ('pull', 0.921419084072113),\n",
       " ('martyr', 0.915220320224762),\n",
       " ('watchers', 0.903829038143158),\n",
       " ('mccoy', 0.9032540321350098),\n",
       " ('liar', 0.9026849269866943),\n",
       " ('lousy', 0.8959661722183228),\n",
       " ('friend:', 0.8959379196166992),\n",
       " ('ventura:', 0.8947344422340393)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('play')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('schedule', 0.9217237234115601),\n",
       " ('closest', 0.8835597038269043),\n",
       " ('cinema', 0.8757786154747009),\n",
       " ('nearest', 0.8755965828895569),\n",
       " ('movies', 0.8676316142082214),\n",
       " ('showtimes', 0.8673536777496338),\n",
       " ('schedules', 0.8636676073074341),\n",
       " ('showing', 0.8634284138679504),\n",
       " ('times', 0.8582863807678223),\n",
       " ('films', 0.8550107479095459)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
