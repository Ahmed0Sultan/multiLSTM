{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate NER data and bootstrap\n",
    "\n",
    "1. create NER labels using rule-based system\n",
    "2. bootstrap sentences by replacing entities using gazetteers\n",
    "3. save sentences as `numpy` vectors for `keras` training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from dataset import get_vocab, index_sents\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../00_data/merged_corpus.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eb5f1df19363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# data = pd.read_csv('data/merged_corpus_phrases_new_fixed.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../00_data/merged_corpus.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../00_data/merged_corpus.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# data = pd.read_csv('data/merged_corpus_phrases_new_fixed.csv')\n",
    "data = pd.read_csv('../00_data/merged_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to str due to number-only lines\n",
    "sents      = [str(s) for s in data['message'].tolist()]\n",
    "speechacts = data['speech_act'].tolist()\n",
    "topics     = [str(s) for s in data['topic'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix 'p m'\n",
    "fixed = []\n",
    "for idx, sent in enumerate(sents):\n",
    "    sent = sent.replace(' p m', ' pm')\n",
    "    sent = sent.replace(' p m.', ' pm.')\n",
    "    sent = sent.replace(' p m?', ' pm?')\n",
    "    sent = sent.replace(' p m,', ' pm,')\n",
    "    sent = sent.replace(' p m ', ' pm ')\n",
    "    sent = sent.replace(' a m.', ' am.')\n",
    "    sent = sent.replace(' a m?', ' am?')\n",
    "    sent = sent.replace(' a m,', ' am,')\n",
    "    sent = sent.replace(' a m ', ' am ')\n",
    "    sent = re.sub('a\\sm$', 'am', sent)\n",
    "    fixed.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and pos-tag with NLTK\n",
    "postagged = [pos_tag(word_tokenize(s)) for s in fixed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(l) for l in postagged]\n",
    "max(lens), (sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for sent in fixed:\n",
    "    words = re.findall(r'[A-Z][a-z]+', sent)\n",
    "    for word in words:\n",
    "        names.append(word)\n",
    "\n",
    "nameCounts = Counter(names)\n",
    "nameCounts.most_common(10) # remove 10 to see all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for sent in fixed:\n",
    "    words = re.findall(r'[A-Z][A-Z]+', sent)\n",
    "    for word in words:\n",
    "        names.append(word)\n",
    "\n",
    "nameCounts = Counter(names)\n",
    "nameCounts.most_common(10) # remove 10 to see all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER auto-tagging\n",
    "\n",
    "this code is messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lists\n",
    "timewords = ['midnight', 'noon', 'am', 'pm', 'morning', 'afternoon', 'evening', 'night']\n",
    "\n",
    "datewords = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'January', 'February',\n",
    "             'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December',\n",
    "             'tomorrow']\n",
    "\n",
    "# from transdict finds\n",
    "placewords = ['Euston', 'London', 'Birmingham', 'Street', 'Chicago', 'Wilmslow', 'Macclesfield', 'Stockport',\n",
    "              'Piedmont', 'Dallas', 'Newark', 'Wigan', 'Preston', 'Denver', 'Liverpool', 'Seattle', 'Tokyo',\n",
    "              'Wrexham', 'Richmond' , 'Manchester', 'Crewe', 'Baltimore', 'Ottawa', 'Toronto', 'Vancouver', \n",
    "              'Moscow', 'Boston', 'Edinburgh', 'Oakland', 'Newcastle', 'Durham', 'Lime', 'Taunton', \n",
    "              'Copenhagen', 'Heathrow', 'Helsinki', 'Pittsburg', 'Raleigh', 'Picadilly', 'Watford', 'Hertford',\n",
    "              'Leicester', 'Newton', 'Abbot', 'Greenbay', 'Miami', 'Orlando', 'Washington', 'Dulles', 'Charlotte',\n",
    "              'Tahoe', 'Southbend', 'Springfield', 'York', 'Burbank', 'Syracuse', 'Cleveland', 'Fairbanks',\n",
    "              'Carolina', 'Montreal', 'Wolverhampton', 'Leeds', 'Derby', 'Blackpool', 'Oxenholme', 'Ontario',\n",
    "              'Riyadh', 'Portland', 'Barclay', 'Calgary', 'Bangkok', 'Burigan', 'Nantucket', 'Menlo', 'Cottage',\n",
    "              'Wisconsin', 'Gatwick', 'Singapore', 'Irvine', 'Frankfurt', 'Jersey', 'Columbus', 'Merseyside', \n",
    "              'Fanshawe', 'Essex', 'Stafford', 'Philadelphia', 'Switzerland', 'Denmark', 'Sandestrom',\n",
    "              'Braniff', 'Stockholm', 'Sweden', 'Germany', 'Belgium', 'Brussels', 'Rochester', 'Anchorage', \n",
    "              'California', 'Arkansas', 'England', 'Michigan', 'Detroit', 'Indiana', 'Louisville', 'Ohio',\n",
    "              'Tulsa', 'Indianapolis', 'Milwaukee', 'Oklahoma', 'Colorado', 'Virginia', 'Coxhoe', 'Redwich',\n",
    "              'Camden', 'Leicestershire', 'Cumbria', 'Heswall', 'Wirral', 'Liver', 'Cheshire', 'Goldhampton',\n",
    "              'International', 'Airport', 'Central', 'General', 'St', 'Atlanta', 'Nuneaton', 'Beijing', 'Europe',\n",
    "              'LA', 'ORD', 'SFO', 'JFK', 'LAX', 'SAS', 'SF', 'Pancreas', 'Rugby']\n",
    "\n",
    "# from transdict finds\n",
    "companywords = ['United', 'American', 'Virgin', 'Express', 'Delta', 'Trainlines', 'Visa', 'Airlines', 'Travel',\n",
    "                'Hertz', 'Lufthansa', 'Northwest', 'Canadian', 'British', 'Trainline', 'Northwestern',\n",
    "                'Mastercard', 'Airline', 'Southwestern', 'Telesales', 'Merchandising', 'Aeroflight',\n",
    "                'Airways', 'Korean', 'Hotel', 'Alaskan', 'Alaska', 'TWA', 'Eagle', 'Saudia', 'Arabian', \n",
    "                'PanAm', 'CP', 'Air', 'trainlines', 'Continental']\n",
    "\n",
    "# seat-class words\n",
    "ticketwords = ['Coach', 'Business', 'non-stop', 'Round-trip', 'Advance',\n",
    "               'Super', 'Value', 'Return', 'Saver', 'First', 'Class', 'Single']\n",
    "\n",
    "# names\n",
    "namewords = ['Sandra']\n",
    "\n",
    "# if number + these = time\n",
    "timeparts = [\"o'clock\"]\n",
    "\n",
    "# split names\n",
    "placefirsts = ['Palo', 'Green', 'New', 'Lime', 'Orange', 'San', 'Los', 'Hong', 'John', 'Las', 'Saudi', 'Saint',\n",
    "               'Santa', 'Soviet', 'Little']\n",
    "placelasts = ['Alto', 'Bay', 'Lime' 'Street', 'County', 'Francisco', 'Angeles', 'Diego', 'Jose', 'Bernadino',\n",
    "              'Kong', 'Wayne', 'Vegas', 'Arabia', 'Louis', 'Petersburg', 'Ana', 'Union', 'Rock']\n",
    "\n",
    "compfirsts = ['Cathay', 'Pan']\n",
    "complasts = ['Pacific', 'Am']\n",
    "\n",
    "currencies = ['pounds', 'dollars', 'cents', 'dollar']\n",
    "\n",
    "numbers = ['hundred', 'hundreds', 'thousand', 'thousands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetag = 'TIM'\n",
    "datetag = 'DAT'\n",
    "citytag = 'GEO'\n",
    "comptag = 'COM'\n",
    "classtag = 'TIX'\n",
    "flighttag = 'FLT'\n",
    "currtag = 'CUR'\n",
    "numtag = 'NUM'\n",
    "\n",
    "# to save\n",
    "alltoks = []\n",
    "alltags = []\n",
    "allners = []\n",
    "\n",
    "# to print\n",
    "makecsv = []\n",
    "\n",
    "indices = []\n",
    "\n",
    "# iterate\n",
    "for jdx, seq in enumerate(postagged):\n",
    "    \n",
    "    # initialize\n",
    "    toks = [t[0] for t in seq]\n",
    "    tags = [t[1] for t in seq]\n",
    "    ners = ['O' for i in range(len(toks))]\n",
    "\n",
    "    # iterate and list-replace\n",
    "    for idx, tok in enumerate(toks[:-1]):\n",
    "        # time words using list substitution\n",
    "        if tok in timewords:\n",
    "            ners[idx] = timetag\n",
    "        # date words\n",
    "        elif tok in datewords:\n",
    "            ners[idx] = datetag\n",
    "        # date words\n",
    "        elif tok in currencies:\n",
    "            ners[idx] = currtag\n",
    "        # place names using list substitution\n",
    "        elif tok in placewords:\n",
    "            ners[idx] = citytag\n",
    "        # company names using list substitution\n",
    "        elif tok in companywords:\n",
    "            ners[idx] = comptag\n",
    "        # ticket class words\n",
    "        elif tok in ticketwords:\n",
    "            ners[idx] = classtag\n",
    "        \n",
    "        # o'clock time using regex\n",
    "        if re.match(r'[0-9]+', tok):\n",
    "            if toks[idx+1] in timeparts:\n",
    "                ners[idx] = timetag\n",
    "                ners[idx+1] = timetag\n",
    "         \n",
    "        # try flight numbers\n",
    "        if tok == 'flight' and re.findall(r'[0-9]', toks[idx+1]):\n",
    "            ners[idx] = flighttag\n",
    "        \n",
    "        # time re search\n",
    "        if re.findall(r'[0-9]+\\:[0-9]+(\\.\\,\\?)*', tok):\n",
    "            ners[idx] = timetag    \n",
    "        \n",
    "        # else assume flight number unless before prep\n",
    "        if ners[idx] == 'O' and re.match(r'[0-9]+', tok):\n",
    "            if toks[idx+1] in ['hours','minutes','miles','kilometers', 'person', 'persons', 'people', 'members']:\n",
    "                ners[idx] = numtag\n",
    "            elif toks[idx+1] in currencies:\n",
    "                ners[idx] = currtag\n",
    "            elif toks[idx+1] in timewords or toks[idx+1] in timeparts:\n",
    "                ners[idx] = timetag\n",
    "            elif idx > 0:\n",
    "                if toks[idx-1] in currencies:\n",
    "                    ners[idx] = currtag\n",
    "                elif ners[idx-1] == flighttag or ners[idx-1] == comptag:\n",
    "                    ners[idx] = flighttag\n",
    "                else:\n",
    "                    ners[idx] = numtag\n",
    "            else:\n",
    "                ners[idx] = numtag\n",
    "        \n",
    "        # ordinal numbers for dates\n",
    "        if re.findall(r'[0-9]+(nd|st|th|rd)', tok):\n",
    "            ners[idx] = datetag\n",
    "                \n",
    "        # two-word place names\n",
    "        if tok in placefirsts:\n",
    "            if toks[idx+1] in placelasts:\n",
    "                ners[idx] = citytag\n",
    "                ners[idx+1] = citytag\n",
    "                \n",
    "        # two-word company names\n",
    "        if tok in compfirsts:\n",
    "            if toks[idx+1] in complasts:\n",
    "                ners[idx] = comptag\n",
    "                ners[idx+1] = comptag\n",
    "                \n",
    "        # ticket class words\n",
    "        elif tok in timeparts:\n",
    "            ners[idx-1] = timetag\n",
    "            ners[idx] = timetag\n",
    "                \n",
    "    # last word\n",
    "    # time words using list substitution\n",
    "    if toks[-1] in timewords:\n",
    "        ners[-1] = timetag\n",
    "    # date words\n",
    "    elif toks[-1] in datewords:\n",
    "        ners[-1] = datetag\n",
    "    elif toks[-1] in numbers:\n",
    "        ners[-1] = numtag\n",
    "    elif toks[-1] in currencies:\n",
    "        ners[-1] = currtag\n",
    "    # place names using list substitution\n",
    "    elif toks[-1] in placewords:\n",
    "        ners[-1] = citytag\n",
    "    # company names using list substitution\n",
    "    elif toks[-1] in companywords:\n",
    "        ners[-1] = comptag\n",
    "    # ticket class words\n",
    "    elif toks[-1] in ticketwords:\n",
    "        ners[-1] = classtag\n",
    "    # time re search\n",
    "    if re.match(r'[0-9]+\\:[0-9]+(\\.\\,\\?)*', toks[-1]):\n",
    "        ners[-1] = timetag \n",
    "    # ordinal numbers for dates\n",
    "    elif re.findall(r'[0-9]+(nd|st|th|rd)', toks[-1]):\n",
    "        ners[-1] = datetag\n",
    "    elif len(seq) > 1 and re.match(r'[0-9]+', toks[-1]): \n",
    "        if ners[-2] == flighttag or ners[-2] == numtag:\n",
    "            ners[-1] = ners[-2]\n",
    "        elif toks[-1] in currencies:\n",
    "            ners[-1] = currtag\n",
    "        elif tags[-2] == 'IN':\n",
    "            ners[-1] = timetag\n",
    "        else:\n",
    "            ners[-1] = numtag\n",
    "    \n",
    "    # final pass\n",
    "    for i in range(len(toks)):\n",
    "        if i < len(toks)-1:\n",
    "            if toks[i] == 'good' and toks[i+1] == 'morning' or toks[i+1] == 'afternoon':\n",
    "                ners[i+1] = 'O'        \n",
    "    \n",
    "    answ = pd.DataFrame(\n",
    "    {\n",
    "        'toks': toks,\n",
    "        'tags': tags,\n",
    "        'ners': ners,\n",
    "        'skip' : [' ' for s in toks]\n",
    "    })\n",
    "\n",
    "    answ = answ[['toks', 'ners', 'skip']]\n",
    "    answ = answ.T\n",
    "    \n",
    "    # add to test data\n",
    "    alltoks.append(toks)\n",
    "    alltags.append(tags)\n",
    "    allners.append(ners)\n",
    "    indices.append(jdx)  # save sentence index\n",
    "    makecsv.append(answ) # append df to list seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [w for s in alltoks for w in s]\n",
    "vcounts = Counter(vocab)\n",
    "vcounts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(makecsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in postagged if len(x)> 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER training prototype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speechacts), len(alltoks), len(alltags), len(allners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saCounter = Counter(speechacts)\n",
    "len(set(speechacts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the speech-act prefixes\n",
    "truncsa = [sa.split('-')[-1] for sa in speechacts]\n",
    "tsaCounter = Counter(truncsa)\n",
    "len(set(truncsa)), tsaCounter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the topic prefixes, suffixes\n",
    "tpCounter = Counter(topics)\n",
    "trunctop = [t.split('-')[-1] for t in topics]\n",
    "trunctop = [t.split('_')[0] for t in trunctop]\n",
    "trunctopCounter = Counter(trunctop)\n",
    "len(set(topics)), len(set(trunctop)), trunctopCounter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicSA = [truncsa[i]+'-'+trunctop[i] for i in range(len(topics))]\n",
    "topicSACounter = Counter(topicSA)\n",
    "len(set(topicSA)) #, topicSACounter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by length, remove all of length 2 or under\n",
    "indices = []\n",
    "for idx, tokseq in enumerate(alltoks):\n",
    "    if len(tokseq) > 2:\n",
    "        indices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortTops = [topicSA[i] for i in indices]\n",
    "shortTopCounter = Counter(shortTops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_speechacts = [truncsa[i] for i in indices]\n",
    "short_topics = [trunctop[i] for i in indices]\n",
    "short_topic_acts = shortTops[:]\n",
    "short_tokens = [alltoks[i] for i in indices]\n",
    "short_postags = [alltags[i] for i in indices]\n",
    "short_nertags = [allners[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supplement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "          'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "cities = ['Chicago', 'Wilmslow', 'Macclesfield', 'Stockport',\n",
    "          'Piedmont', 'Dallas', 'Newark', 'Wigan', 'Preston', 'Denver', 'Liverpool', 'Seattle', 'Tokyo',\n",
    "          'Wrexham', 'Richmond' , 'Manchester', 'Crewe', 'Baltimore', 'Ottawa', 'Toronto', 'Vancouver', \n",
    "          'Moscow', 'Boston', 'Edinburgh', 'Oakland', 'Newcastle', 'Durham', 'Taunton', \n",
    "          'Copenhagen', 'Helsinki', 'Pittsburg', 'Raleigh', 'Picadilly', 'Watford',\n",
    "          'Leicester', 'Newton', 'Abbot', 'Greenbay', 'Miami', 'Orlando', 'Charlotte',\n",
    "          'Tahoe', 'Southbend', 'Springfield', 'Burbank', 'Syracuse', 'Cleveland', 'Fairbanks',\n",
    "          'Montreal', 'Wolverhampton', 'Leeds', 'Derby', 'Blackpool', 'Oxenholme', 'Ontario',\n",
    "          'Riyadh', 'Portland', 'Barclay', 'Calgary', 'Bangkok', 'Burigan', 'Nantucket', 'Menlo', 'Cottage',\n",
    "          'Gatwick', 'Singapore', 'Irvine', 'Frankfurt', 'Jersey', 'Columbus', 'Merseyside', \n",
    "          'Fanshawe', 'Essex', 'Stafford', 'Philadelphia', 'Sandestrom',\n",
    "          'Braniff', 'Stockholm', 'Brussels', 'Rochester', 'Anchorage', \n",
    "          'Detroit', 'Indiana', 'Louisville',\n",
    "          'Tulsa', 'Indianapolis', 'Milwaukee', 'Oklahoma', 'Coxhoe', 'Redwich',\n",
    "          'Camden', 'Leicestershire', 'Cumbria', 'Heswall', 'Wirral', 'Liver', 'Goldhampton',\n",
    "          'Atlanta', 'Nuneaton', 'Beijing']\n",
    "\n",
    "newcities = ['Shanghai', 'Seoul', 'Busan', 'Daegu', 'Paris', 'Istanbul', 'Delhi', 'Mumbai', 'Cairo',\n",
    "             'Tehran', 'Ankara', 'Berlin', 'Madrid', 'Incheon', 'Rome', 'Osaka',\n",
    "             'Nagoya', 'Manila', 'Minsk', 'Budapest', \"Warsaw\", \"Phoenix\"]\n",
    "\n",
    "airlines = ['United', 'Virgin', 'Delta', 'Trainlines',\n",
    "            'Hertz', 'Lufthansa', 'Northwest', 'Canadian', 'British', 'Northwestern',\n",
    "            'Airline', 'Southwestern', 'Aeroflight', 'Continental',\n",
    "            'Airways', 'Korean', 'Alaskan', 'Alaska', 'TWA', 'Eagle', 'Saudia', 'Arabian', \n",
    "            'PanAm', 'CP', 'Air']\n",
    "\n",
    "newairlines = ['Allegiant', 'Frontier', 'JetBlue', 'Spirit', 'Envoy', 'SkyWest', 'Quantas',\n",
    "               'Etihad', 'Dragonair', 'AirAsia', 'Finnair', 'Aegean', 'easyJet', 'WestJet',\n",
    "               'Indigo', 'Iberia', 'Emirates', 'EVA', 'ANAs', 'JAL']\n",
    "\n",
    "journey = ['journey', 'trip', 'itinerary']\n",
    "\n",
    "traveling = ['travelling', 'traveling', 'flying']\n",
    "\n",
    "depart = ['depart', 'leave']\n",
    "\n",
    "departing = ['departing', 'leaving']\n",
    "\n",
    "book = ['book', 'schedule', 'reserve']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_speechacts = []\n",
    "add_topics = []\n",
    "add_topic_acts = []\n",
    "add_tokens = []\n",
    "add_postags = []\n",
    "add_nertags = []\n",
    "\n",
    "for i in range(len(short_speechacts)):\n",
    "    \n",
    "    iters = np.random.randint(4, 8)\n",
    "    for j in range(iters):\n",
    "    \n",
    "        toks = short_tokens[i]\n",
    "        tags = short_postags[i]\n",
    "        ners = short_nertags[i]\n",
    "\n",
    "        new_toks = toks[:]\n",
    "\n",
    "        # check each list\n",
    "        for lst in [days, months, cities, airlines, journey, traveling, depart, departing, book]:\n",
    "\n",
    "            # check each word\n",
    "            for idx, tok in enumerate(toks):\n",
    "\n",
    "                # if found, randomly sample new word\n",
    "                if tok in lst:\n",
    "                    new = tok\n",
    "                    while new == tok:\n",
    "                        if lst == cities:\n",
    "                            new = newcities[np.random.randint(0, len(newcities))]\n",
    "                        elif lst == airlines:\n",
    "                            new = newairlines[np.random.randint(0, len(newairlines))]\n",
    "                        else:\n",
    "                            new = lst[np.random.randint(0, len(lst))]\n",
    "                    # replace\n",
    "                    new_toks[idx] = new\n",
    "        \n",
    "        # update if new sent\n",
    "        if toks != new_toks:\n",
    "            add_tokens.append(new_toks)\n",
    "            add_postags.append(short_postags[i])\n",
    "            add_nertags.append(short_nertags[i])\n",
    "            add_speechacts.append(short_speechacts[i])\n",
    "            add_topics.append(short_topics[i])\n",
    "            add_topic_acts.append(short_topic_acts[i])\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"done\", i, \"of\", len(short_speechacts))\n",
    "        \n",
    "len(add_tokens), len(add_postags), len(add_nertags), len(add_speechacts), len(add_topics), len(add_topic_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100,105):\n",
    "    print(add_tokens[i])\n",
    "    print(add_postags[i])\n",
    "    print(add_nertags[i])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_speechacts += short_speechacts\n",
    "add_topics += short_topics\n",
    "add_topic_acts += short_topic_acts\n",
    "add_tokens += short_tokens\n",
    "add_postags += short_postags\n",
    "add_nertags += short_nertags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(lol):\n",
    "    result = []\n",
    "    for lst in lol:\n",
    "        this_sent = []\n",
    "        for w in lst:\n",
    "            for number in ['1','2','3','4','5','6','7','8','9','0']:\n",
    "                w = w.replace(number, '#')\n",
    "            this_sent.append(w.lower())\n",
    "        result.append(this_sent)\n",
    "    return result\n",
    "\n",
    "add_tokens = lowercase(add_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "np.random.seed(1337)\n",
    "sindices = list(np.random.permutation(len(add_speechacts)))\n",
    "\n",
    "add_speechacts = [add_speechacts[i] for i in sindices]\n",
    "add_topics = [add_topics[i] for i in sindices]\n",
    "add_topic_acts = [add_topic_acts[i] for i in sindices]\n",
    "add_tokens = [add_tokens[i] for i in sindices]\n",
    "add_postags = [add_postags[i] for i in sindices]\n",
    "add_nertags = [add_nertags[i] for i in sindices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sindices[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create vocab and index dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, idx2word = get_vocab(add_tokens, 10000)\n",
    "pos2idx, idx2pos = get_vocab(alltags, len(set([tag for sent in add_postags for tag in sent]))+2)\n",
    "ner2idx, idx2ner = get_vocab(allners, len(set([tag for sent in add_nertags for tag in sent]))+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../00_data/encoded/word2idx.npy', word2idx)\n",
    "np.save('../00_data/encoded/idx2word.npy', idx2word)\n",
    "np.save('../00_data/encoded/pos2idx.npy', pos2idx)\n",
    "np.save('../00_data/encoded/idx2pos.npy', idx2pos)\n",
    "np.save('../00_data/encoded/ner2idx.npy', ner2idx)\n",
    "np.save('../00_data/encoded/idx2ner.npy', idx2ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../00_data/encoded/add_speechacts.npy', add_speechacts)\n",
    "np.save('../00_data/encoded/add_topics.npy', add_topics)\n",
    "np.save('../00_data/encoded/add_topic_acts.npy', add_topic_acts)\n",
    "np.save('../00_data/encoded/add_tokens.npy', add_tokens)\n",
    "np.save('../00_data/encoded/add_postags.npy', add_postags)\n",
    "np.save('../00_data/encoded/add_nertags.npy', add_nertags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas",
   "language": "python",
   "name": "atlas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
