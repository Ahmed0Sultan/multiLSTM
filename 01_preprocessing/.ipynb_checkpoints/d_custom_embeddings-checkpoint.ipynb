{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create custom word2vec embeddings\n",
    "\n",
    "use a mix of in-domain data (= the merged corpus sentences) and general domain data from the Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataset import get_vocab, index_sents\n",
    "from embedding import create_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in-domain text - lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in in-domain text, POS-tags\n",
    "alltoks = list(np.load('../00_data/encoded/add_tokens.npy'))\n",
    "alltags = list(np.load('../00_data/encoded/add_postags.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_toks = []\n",
    "for seq in alltoks:\n",
    "    seq = [str(s) for s in seq]\n",
    "    if len(seq) > 1:\n",
    "        string_toks.append(' '.join(seq).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_tags = []\n",
    "for seq in alltags:\n",
    "    seq = [str(s) for s in seq]\n",
    "    if len(seq) > 1:\n",
    "        string_tags.append(' '.join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15513"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supplement with generic text - lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12403"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brownsents = brown.sents(categories=['news', 'editorial', 'reviews', 'government'])\n",
    "brownsents = [' '.join(s).lower() for s in brownsents]\n",
    "len(brownsents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generic token for number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_text = brownsents + string_toks\n",
    "\n",
    "for idx, sent in enumerate(sentence_text):\n",
    "    for number in ['1','2','3','4','5','6','7','8','9','0']:\n",
    "        sent = sent.replace(number, '#')\n",
    "    sentence_text[idx] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21983,\n",
       " [('the', 23000),\n",
       "  (',', 13677),\n",
       "  ('to', 11078),\n",
       "  ('of', 10966),\n",
       "  ('.', 10553),\n",
       "  ('and', 9721),\n",
       "  ('a', 7573),\n",
       "  ('in', 6528),\n",
       "  ('that', 4928),\n",
       "  ('#', 4795)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lists = [s.split() for s in sentence_text]\n",
    "vocab = [w for s in sent_lists for w in s]\n",
    "from collections import Counter\n",
    "vset = set(vocab)\n",
    "vcount = Counter(vocab)\n",
    "len(vset), vcount.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings\n",
    "\n",
    "with open('../00_data/embeddings/sent_text.txt', 'w') as f:\n",
    "    for s in sentence_text:\n",
    "        f.write(s)\n",
    "        f.write('\\n')\n",
    "\n",
    "w2v_vocab, w2v_model = create_embeddings('../00_data/embeddings/sent_text.txt',\n",
    "                       embeddings_path='../00_data/embeddings/text_embeddings.gensimmodel',\n",
    "                       vocab_path='../00_data/embeddings/text_mapping.json',\n",
    "                       min_count=1,\n",
    "                       workers=4,\n",
    "                       size = 160,\n",
    "                       iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postag embeddings\n",
    "\n",
    "with open('../00_data/embeddings/postag_text.txt', 'w') as f:\n",
    "    for s in string_tags:\n",
    "        f.write(s)\n",
    "        f.write('\\n')\n",
    "\n",
    "w2v_pvocab, w2v_pmodel = create_embeddings('../00_data/embeddings/postag_text.txt',\n",
    "                         embeddings_path='../00_data/embeddings/postag_embeddings.gensimmodel',\n",
    "                         vocab_path='../00_data/embeddings/postag_mapping.json',\n",
    "                         min_count=3,\n",
    "                         workers=4,\n",
    "                         size=32,\n",
    "                         iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mumbai', 0.546605110168457),\n",
       " ('nuneaton', 0.5448371171951294),\n",
       " ('manila', 0.5345560312271118),\n",
       " ('seoul', 0.5336037874221802),\n",
       " ('budapest', 0.5319229960441589),\n",
       " ('cairo', 0.5309597253799438),\n",
       " ('madrid', 0.5206683874130249),\n",
       " ('minsk', 0.5083528161048889),\n",
       " ('daegu', 0.49895063042640686),\n",
       " ('stockport', 0.49597442150115967)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('london')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('february', 0.7360303997993469),\n",
       " ('november', 0.7329068183898926),\n",
       " ('september', 0.7270194888114929),\n",
       " ('january', 0.7232068777084351),\n",
       " ('april', 0.7010664343833923),\n",
       " ('december', 0.6981652975082397),\n",
       " ('august', 0.6908720135688782),\n",
       " ('march', 0.6824270486831665),\n",
       " ('june', 0.6813216805458069),\n",
       " ('july', 0.6104220151901245)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('october')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('##:##', 0.6680556535720825),\n",
       " ('madrid', 0.5936460494995117),\n",
       " ('flight', 0.5899808406829834),\n",
       " ('nagoya', 0.5555135011672974),\n",
       " ('osaka', 0.5316229462623596),\n",
       " ('mumbai', 0.5308681130409241),\n",
       " ('tehran', 0.5255060791969299),\n",
       " ('daegu', 0.515431821346283),\n",
       " ('manila', 0.5147073864936829),\n",
       " ('busan', 0.5084124803543091)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('#:##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"####'s\", 0.37837108969688416),\n",
       " ('ending', 0.36077120900154114),\n",
       " ('year', 0.34104183316230774),\n",
       " ('sales', 0.3399466276168823),\n",
       " ('y.', 0.33605659008026123),\n",
       " ('months', 0.33141085505485535),\n",
       " ('rose', 0.32533833384513855),\n",
       " ('#%', 0.32455965876579285),\n",
       " ('bills', 0.31682369112968445),\n",
       " ('filed', 0.3165268898010254)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FW', 0.5746904611587524),\n",
       " ('JJ', 0.497397780418396),\n",
       " ('UH', 0.49557361006736755),\n",
       " ('WP$', 0.46631431579589844),\n",
       " ('PRP', 0.463824987411499),\n",
       " ('WRB', 0.4284180700778961),\n",
       " ('NNS', 0.41058939695358276),\n",
       " ('VB', 0.3847822844982147),\n",
       " ('CC', 0.3781307339668274),\n",
       " ('RB', 0.35533303022384644)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_pmodel.most_similar('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core_eng",
   "language": "python",
   "name": "core_eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
